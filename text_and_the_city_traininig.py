# -*- coding: utf-8 -*-
"""text-and-the-city-traininig.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18MCWEFqhBeihmWWVfzt2sTlI-30gKsNk

# Text and the City - Training

This notebook demonstrates how to fine-tune a BERT-like model for multi-label classification using an Excel file containing the "Text and the City" data.

Note: This notebook does not cover the extraction of content from PDF files. For that, please refer to the other provided notebook.

The notebook was executed on a free instance of Google Colab with 12.7 GB RAM, but it can be readily adapted to a regular Jupyter Notebook that can be run in any suitable environment.

## Install dependencies
"""

!pip install -q transformers[torch] datasets

"""## Load Data

To access your documents on Google Drive, you must grant Google Colab permission to your drive.

In the following code, we assume that you have a file named `10_Aufgabe_Innovationswettbewerb.xlsx` containing the required data. Feel free to change the file name as needed.
"""

from google.colab import drive
import os

drive.mount('/content/drive')

DRIVE_DIR = "/content/drive/MyDrive/"

EXEL_FILE_NAME = "10_Aufgabe_Innovationswettbewerb.xlsx"

assert EXEL_FILE_NAME in os.listdir(DRIVE_DIR)

"""Create a pandas DataFrame from the Excel file."""

import pandas as pd

xlsx_path = DRIVE_DIR + EXEL_FILE_NAME

df = pd.read_excel(xlsx_path, engine='openpyxl')
df

"""## Data Cleaning

Remove rows where the "Name des pdf Dokuments" (PDF names) column is not provided.
"""

df = df[df['Name des pdf Dokuments'].notnull()]

assert len(df[df['Inhalt'].isnull()].values) == 0
df

"""## Extract Labels

Retrieve all unique "Ziel" (goal) values from the dataframe and map each one to an index.
"""

labels = [label for label in df['Ziel'].unique()]
id2label = {idx:label for idx, label in enumerate(labels)}
label2id = {label:idx for idx, label in enumerate(labels)}
print("Number of labels: ", len(labels))
labels

"""## Label Data

Retain only the "Name des pdf Dokuments" and "Inhalt" (content) columns from the dataframe and eliminate duplicate entries. Create a new column for each document named "labels", containing an array. The array has a length equal to the number of unique goals. Set the value to 1 when the index corresponds to the goal associated with that document and 0 otherwise.
"""

import numpy as np

no_duplicated_df = df.drop_duplicates(subset=['Name des pdf Dokuments'])[['Name des pdf Dokuments', "Inhalt"]]
#no_duplicated_df["label"] = [np.array([1 if len(df[('Name des pdf Dokuments' == name) & ("Ziel" == label)].values)else 0 for label in labels]) for name in no_duplicated_df['Name des pdf Dokuments'].values]

def create_labels_column(row):
  return [1.0 if len(df[(df['Name des pdf Dokuments'] == row['Name des pdf Dokuments']) & (df["Ziel"] == label)].values)else 0.0 for label in labels]

no_duplicated_df['labels'] = no_duplicated_df.apply(create_labels_column, axis=1)
no_duplicated_df

"""Rename the "Inhalt" column to "text" and retain only the "text" and "labels" columns.

"""

df = no_duplicated_df.rename(columns={'Inhalt': 'text'})[["text", "labels"]]
df = df.reset_index(drop=True)
df

"""## Split Dataset into Training and Validation

> NOTE:
>
> Since the least populated class in y has only 1 member, the dataset cannot be split in a stratified way. If more datapoints are available, try uncommenting `stratify=df["labels"].values`.
"""

import sklearn.model_selection

indices_train, indices_test = sklearn.model_selection.train_test_split(
    np.arange(df.shape[0]),
    random_state=42,
    shuffle=True,
    test_size=0.2,
    # stratify=df["labels"].values,
)

"""## Choose Model

Specify the name of a model available on Hugging Face.

Some options:
*   `microsoft/mdeberta-v3-base`
*   `bert-base-multilingual-cased`

Visit [this link](https://huggingface.co/models) for additional models. You can filter for language `German` and task `Classification` to explore other models.


"""

HUGGING_FACE_NAME = "bert-base-multilingual-cased"

"""## Load Tokenizer"""

from transformers import AutoTokenizer
import numpy as np

tokenizer = AutoTokenizer.from_pretrained(HUGGING_FACE_NAME)

"""Convert the pandas dataframe into a tokenized dataset that can be directly input into the model.

> NOTE:
>
> The tokenizer truncates the vector representation of texts with more than 512 tokens, as this is the limit for BERT-like models. It might be worthwhile to explore whether the model performance improves by creating more data points using sliding/tumbling window techniques for texts exceeding 512 tokens. Conversely, if a text is represented in fewer than 512 tokens, padding is added at the end of the vector representation.
"""

import functools
import datasets

def tok_func(x, tokenizer):
    """
    tokenizes the field 'text' stored in x including padding
    """
    bert_max_token_n = 512
    return tokenizer(x["text"], padding="max_length", truncation=True, max_length=bert_max_token_n)

def get_dataset(ds, tok_func, tokenizer, indices_train, indices_test, train=True):
    """
    converts dataset to 'dataset' format required by Hugging Face

    Parameters:
    ----------
    ds: dataset
    tok_func: functiond use for tokenization
    indices_train: indices corresponding to the training set
    indices_test: indices corresponding to the training set
    train: if used for training

    Returns
    -------
    header of file
    """
    datasets_ds = datasets.Dataset.from_pandas(df)
    tok_function_partial = functools.partial(tok_func, tokenizer=tokenizer)
    tok_ds = datasets_ds.map(tok_function_partial, batched=True)
    if train:
        return datasets.DatasetDict(
            {
                "train": tok_ds.select(indices_train),
                "test": tok_ds.select(indices_test),
            }
        )
    else:
        return tok_ds

# create Hugging Face 'dataset'
dataset = get_dataset(df, tok_func, tokenizer, indices_train, indices_test)

"""### Example of a Datapoint"""

example = dataset['train'][0]
print(example.keys())

tokenizer.decode(example['input_ids'])

example['labels']

"""## Load Model"""

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(HUGGING_FACE_NAME,
                                                           problem_type="multi_label_classification",
                                                           num_labels=len(labels),
                                                           id2label=id2label,
                                                           label2id=label2id)

"""## Choose Hyperparameters

These parameters can be adjusted to achieve a more accurate model at the end of training.
"""

parameters = {}
parameters["learning_rate"] = 8e-5
parameters["batch_size"] = 8
parameters["weight_decay"] = 0.01
parameters["epochs"] = 1
# Could be considered
# parameters["warmup_ratio"] = 0.1
# parameters["cls_dropout"] = 0.3

FOLDER_TO_OUTPUT = "parametes"
parameters

from transformers import TrainingArguments, Trainer

args = TrainingArguments(
    FOLDER_TO_OUTPUT,
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=parameters["learning_rate"],
    per_device_train_batch_size=parameters["batch_size"],
    per_device_eval_batch_size=parameters["batch_size"],
    num_train_epochs=parameters["epochs"],
    weight_decay=parameters["weight_decay"],
    load_best_model_at_end=True,
    metric_for_best_model="f1",
)

"""## Define Metrics

This function defines the metrics used to evaluate the model and how these metrics are computed.

In this case, the f1 score, the ROC curve, and accuracy are computed. It is recommended to refer to the respective Wikipedia pages for more information on these metrics.

The overall metric used is the f1 score with micro average. For additional information and alternatives, refer to [this link](https://towardsdatascience.com/evaluating-multi-label-classifiers-a31be83da6ea).

The overall metric used can be changed; it has to be implemented in the `multi_label_metrics` function, and then the `metric_for_best_model` parameter in the cell above has to be set accordingly.

Keep in mind that the choice of the overall metric significantly influences how the model is trained.

> NOTE:
>
> The threshold value determines how high a value after the last sigmoid function must be to consider it a relevant goal for the given input text. This value can be adjusted as needed.
"""

from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from transformers import EvalPrediction
import torch

# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/
def multi_label_metrics(predictions, labels, threshold=0.5):
    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))
    # next, use threshold to turn them into integer predictions
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= threshold)] = 1
    # finally, compute metrics
    y_true = labels
    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')
    accuracy = accuracy_score(y_true, y_pred)
    # return as dictionary
    metrics = {'f1': f1_micro_average,
               'roc_auc': roc_auc,
               'accuracy': accuracy}
    return metrics

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions,
            tuple) else p.predictions
    result = multi_label_metrics(
        predictions=preds,
        labels=p.label_ids)
    return result

"""## Fine-tune the Model

The model is trained on the training data and subsequently evaluated on the validation data.
"""

trainer = Trainer(
    model,
    args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

"""## Check Overfitting

Evaluate on the training data.

"""

trainer.evaluate(eval_dataset=dataset["train"])

"""In this scenario (actual numbers may vary), the training set's accuracy surpasses that of the validation dataset, indicating potential overfitting.

## Inference

This code can perform inference using the fine-tuned model.
"""

text = "Am 4. Januar 2021 hat das Bundesministerium für Verkehr und digitale Infrastruktur (BMVI) den vierten Förderaufruf zur Förderrichtlinie Städtische Logistik veröffentlicht. Das Ziel der Förderrichtlinie ist es, Landkreise und Kommunen dabei zu unterstützen, optimale Rahmenbedingungen für eine effiziente und nachhaltige städtische Logistik zu schaffen."

encoding = tokenizer(text, return_tensors="pt")
encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

outputs = trainer.model(**encoding)

logits = outputs.logits
logits.shape

# apply sigmoid + threshold
sigmoid = torch.nn.Sigmoid()
probs = sigmoid(logits.squeeze().cpu())
predictions = np.zeros(probs.shape)
predictions[np.where(probs >= 0.5)] = 1
# turn predicted id's into actual label names
predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]
print(predicted_labels)

"""## Final Remarks

*   The current number of labels is 25, resulting in 2<sup>25</sup> = 33,554,432 possible multi-label variations. Clustering the labels may be necessary.
*   The dataset is showing signs of overfitting, with low f1-score and accuracy. Consider augmenting the dataset size, even if further reduction in the number of labels is implemented.
*   Investigating the distribution of labels in the datapoints is crucial. Currently, it may not provide meaningful insights due to the imbalance between the numerous classes and limited datapoints. (histograms, confusion matrices)
*   As previously mentioned, certain aspects can be modified to enhance the model, including the choice of the base model, adjustment of hyperparameters, refining tokenization methods (summarizing before, windowing), fine-tuning thresholds, and optimizing output layer functions.
"""